### Boosting & Bagging的区别：
![](https://assets.che300.com/wiki/2021-01-14/1610605093167-image.png)
![](https://assets.che300.com/wiki/2021-01-14/1610605179970-image.png)

**集成学习对于单个模型的要求是“好而不同”，需要满足diversity。如果单个模型的精度低，那么集成结果也会比较差。**

在实际过程当中，机器学习的”准确性“和”多样性“是 存在冲突的。当模型出现较高的准确性，那么就会丧失多样性。

### Boosting
每个算法都会生成一个弱规则，然后通过多次迭代，boosting算法集合成了强大的决策规则：
1. 基础学习当中每个观测值都有同样的权重
2. 在第一个算法当中产生的错误来修正下一次算法中的权重
3. 一直迭代直到达到指定预期

**Boosting会更加关注错误分类的弱规则**
常用算法：
+ AdaBoost（Adaptive Boosting）
+ GBM（Gradient Boosting Machine）
+ XGBoost


